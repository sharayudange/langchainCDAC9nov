{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Template Classes\n",
    "- Purpose: Provide a mechanism to construct prompts for models. Prompts come in varying shapes and sizes to suit different business needs. Prompt template classes address all these needs and help produce a Prompt Value class that's used as a prompt to models.\n",
    "1. Remember from the PromptValue parameter that's input to all model classes\n",
    "2. This session is all about generating that at runtime in a scalable and flexible way to address diverse business needs.\n",
    "3. Product `PromptValue` from `PromptTemplate`\n",
    "4. The classes that make up prompts module:\n",
    "5. https://techblogs.cloudlex.com/langchain-iii-prompts-2df826c0ec3d#a487\n",
    "6. For chat applications we use `ChatPromptTemplate` the concrete derived class from `BaseChatPromptTemplate`\n",
    "7. If we wish to use f-strings in our prompt template then we use a concrete derived class of `StringPromptTemplate`\n",
    "\n",
    "### Prompt Value Classes\n",
    "- Purpose: Represent the value returned by a Prompt Template when its invoked.\n",
    "1. Represents the actual prompt that sent to the model\n",
    "2. Is generated at runtime when the prompt template is invoked\n",
    "3. This is the class hierarchy: https://techblogs.cloudlex.com/langchain-iii-prompts-2df826c0ec3d#ee36\n",
    "\n",
    "### Message Prompt Template Classes\n",
    "1. Prompts employ messages prompt templates to create and manipulate prompt values at runtime\n",
    "2. Here are the classes: https://techblogs.cloudlex.com/langchain-iii-prompts-2df826c0ec3d#ea75\n",
    "\n",
    "### Message Classes\n",
    "1. These represent the messages themselves.\n",
    "2. They are produced by prompt message templates\n",
    "3. Support for chunking large messages is in these set of classes.\n",
    "4. Here are the classes: https://techblogs.cloudlex.com/langchain-iii-prompts-2df826c0ec3d#e750\n",
    "\n",
    "\n",
    "### NOTE\n",
    "1. The names of the classes in this module can be rather confusing.\n",
    "2. A Prompt is made up of messages\n",
    "3. A class whose name contains *MessagePromptTemplate* usually refers to template for the messages that make up the prompt\n",
    "4. A class whose name contains *PromptTemplate* is the template of the overall prompt.\n",
    "5. Check this diagram: https://techblogs.cloudlex.com/langchain-iii-prompts-2df826c0ec3d#a487\n",
    "\n",
    "Prompt Template Classes -> Prompt Value\n",
    "Message Prompt Template Classes -> Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup done successfully\n",
      "The type of Prompt Message Template is \n",
      "\t<class 'langchain_core.prompts.chat.ChatMessagePromptTemplate'>\n",
      "The type of message is: \n",
      "\t<class 'langchain_core.messages.chat.ChatMessage'>, \n",
      "\tand its __repr__ value is:  ChatMessage(content='Please give me flight options for New Delhi to Mumbai', additional_kwargs={}, response_metadata={}, role='travel agent')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import configparser\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# set up config parser\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"../config.ini\")  # holds secrets and keys\n",
    "\n",
    "\n",
    "# load Groq config\n",
    "groq = config[\"groq\"]\n",
    "os.environ['GROQ_API_KEY'] = groq.get('GROQ_API_KEY')\n",
    "\n",
    "# select the model.\n",
    "model = ChatGroq(model=\"llama3-8b-8192\")\n",
    "\n",
    "print('setup done successfully')\n",
    "\n",
    "# Basic Message Prompt Template\n",
    "from langchain_core.prompts.chat import ChatMessagePromptTemplate\n",
    "\n",
    "# This is the typical factory method\n",
    "# Best is to check source code: https://github.com/langchain-ai/langchain/blob/master/libs/core/langchain_core/prompts/chat.py#L299\n",
    "chatMessagePrompt = ChatMessagePromptTemplate.from_template(\n",
    "    template= \"Please give me flight options for {from_city} to {to_city}\",\n",
    "    role=\"travel agent\"\n",
    ")\n",
    "\n",
    "# In the above role is required.\n",
    "\n",
    "print(f\"The type of Prompt Message Template is \\n\\t{type(chatMessagePrompt)}\")\n",
    "\n",
    "# Use the prompt message template to generate the message by providing values for all the replacement variables.\n",
    "baseMessage = chatMessagePrompt.format(from_city=\"New Delhi\", to_city=\"Mumbai\", role=\"travel agent\")\n",
    "\n",
    "print(\n",
    "    f\"The type of message is: \\n\\t{type(baseMessage)}, \\n\\tand its __repr__ value is:  {baseMessage.__repr__()}\"\n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Lets try sending a system and human message to the model and get the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A great question! The capital of India is New Delhi. New Delhi has been the capital of India since 1911, when it was declared the new capital by the British. Prior to that, Kolkata (then known as Calcutta) was the capital of British India. Since India gained independence in 1947, New Delhi has remained the capital, serving as the seat of the government and the residence of the President of India.\n"
     ]
    }
   ],
   "source": [
    "# Create individual messages\n",
    "sysMessage = SystemMessage(\"You are a chat bot who is an expert in political science\")\n",
    "humanMessage = HumanMessage(content=\"What is the capital of India?\")\n",
    "\n",
    "response = model.invoke([sysMessage, humanMessage])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of the United States of America is Washington, D.C.\n"
     ]
    }
   ],
   "source": [
    "# Same example with a Message Prompt Template.\n",
    "\n",
    "from langchain_core.prompts.chat import (SystemMessagePromptTemplate, HumanMessagePromptTemplate)\n",
    "\n",
    "sysMessage = SystemMessagePromptTemplate.from_template(template='You are a chat bot who is an expert in political science')\n",
    "humanMessage = HumanMessagePromptTemplate.from_template(template='What is the capital of {country}')\n",
    "\n",
    "\n",
    "response = model.invoke([sysMessage.format(), humanMessage.format(country='USA')])\n",
    "\n",
    "print(response.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Prompt Template - using a list of string.\n",
    "This represents a conversation context of a chat. The conversation can contain many back and forth messages. This template threrefore has a colleciton of messages. We can supply the messages using tuples or creating message objects using Message Prompt Template.\n",
    "\n",
    "We can provide any of the following type of messages:\n",
    "1. system - Like instructions or a note or context. Anything that we as the developer want the model to use for understanding its role, its tasks and expectations and give sufficient context.\n",
    "2. human - Encapsulates any responses or messages by the user\n",
    "3. ai - Encapsulates any responses by the model. \n",
    "4. placeholder - A placeholder where the prompt will receive an optional list of messages at run time.\n",
    "5. assistant - Gets created as an AI message internally\n",
    "\n",
    "Apart from this there are message templates to represent messages to tools and their responses that go back to model. We shall cover them when we get to tools.\n",
    "\n",
    "Below is an example of creating a chat prompt using string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Template Messages:  [SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['name'], input_types={}, partial_variables={}, template='You are a helpful AI bot. Your name is {name}.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Hello, how are you doing?'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=\"I'm doing well, thanks!\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['user_input'], input_types={}, partial_variables={}, template='{user_input}'), additional_kwargs={})]\n",
      "Template Object:  ChatPromptTemplate(input_variables=['name', 'user_input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['name'], input_types={}, partial_variables={}, template='You are a helpful AI bot. Your name is {name}.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='Hello, how are you doing?'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template=\"I'm doing well, thanks!\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['user_input'], input_types={}, partial_variables={}, template='{user_input}'), additional_kwargs={})])\n",
      "Prompt Value:  ChatPromptValue(messages=[SystemMessage(content='You are a helpful AI bot. Your name is Arun.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello, how are you doing?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I'm doing well, thanks!\", additional_kwargs={}, response_metadata={}), HumanMessage(content='How is the weather today?', additional_kwargs={}, response_metadata={})])\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "        (\"human\", \"Hello, how are you doing?\"),\n",
    "        (\"ai\", \"I'm doing well, thanks!\"),\n",
    "        (\"human\", \"{user_input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "print('Template Messages: ',template.messages.__repr__())\n",
    "print('Template Object: ', template.__repr__())\n",
    "\n",
    "\n",
    "promptValue = template.invoke(input={\"name\":\"Arun\", \"user_input\":\"How is the weather today?\"})\n",
    "print('Prompt Value: ',promptValue.__repr__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatPrompt Template using objects\n",
    "\n",
    "Lets try and create the same prompt using Message Prompt Templates instead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Value:  ChatPromptValue(messages=[SystemMessage(content='You are a helpful AI bot. Your name is Arun.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello, how are you doing?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I'm doing well, thanks!\", additional_kwargs={}, response_metadata={}), HumanMessage(content='How is the weather today?', additional_kwargs={}, response_metadata={})])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm a digital AI assistant, I don't have the capability to check the current weather conditions. I exist solely in the digital realm and don't have access to real-time information about the physical world. However, I can suggest some ways for you to find out the weather today! You can check your phone's weather app, visit a weather website, or even ask your smart speaker to give you the latest forecast. I hope that helps!\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 90, 'prompt_tokens': 58, 'total_tokens': 148, 'completion_time': 0.075, 'prompt_time': 0.002444426, 'queue_time': 0.011378943, 'total_time': 0.077444426}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_179b0f92c9', 'finish_reason': 'stop', 'logprobs': None}, id='run-840746e8-c019-4649-9892-cf58bee1a704-0', usage_metadata={'input_tokens': 58, 'output_tokens': 90, 'total_tokens': 148})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate, ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "# We use message prompt template as we have a replacement variable.\n",
    "systemMessage = SystemMessagePromptTemplate.from_template(\"You are a helpful AI bot. Your name is {name}.\")\n",
    "\n",
    "# No need to use message prompt template as no replacement variable.\n",
    "humanMessage1 = HumanMessage(\"Hello, how are you doing?\")\n",
    "\n",
    "aimessage = AIMessage(\"I'm doing well, thanks!\")\n",
    "\n",
    "humanMessage2 = HumanMessagePromptTemplate.from_template(\"{user_input}\")\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    systemMessage, humanMessage1, aimessage, humanMessage2\n",
    "])\n",
    "\n",
    "promptValue = template.invoke(input={\"name\":\"Arun\", \"user_input\":\"How is the weather today?\"})\n",
    "\n",
    "print('Prompt Value: ', promptValue.__repr__())\n",
    "\n",
    "model.invoke(promptValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Prompt Template with image\n",
    "Lets say you want to send an iamge to a Model along with a question associated with the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Prompt ImagePromptTemplate(input_variables=['imageName'], input_types={}, partial_variables={}, template={'url': 'http://yourdomain.com/{imageName}', 'detail': 'low'})\n",
      "Prompt Value ImagePromptValue(image_url={'detail': 'low', 'url': 'http://yourdomain.com/good_pic.jpeg'})\n",
      "ImageURL object {'detail': 'low', 'url': 'http://yourdomain.com/good_pic.jpeg'}\n",
      "Human Message1 HumanMessagePromptTemplate(prompt=[ImagePromptTemplate(input_variables=['imageName'], input_types={}, partial_variables={}, template={'url': 'http://yourdomain.com/{imageName}', 'detail': 'low'})], additional_kwargs={})\n",
      "Human Message2 HumanMessage(content='Describe this image in brief', additional_kwargs={}, response_metadata={})\n",
      "Chat prompt:  ChatPromptTemplate(input_variables=['imageName'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=[ImagePromptTemplate(input_variables=['imageName'], input_types={}, partial_variables={}, template={'url': 'http://yourdomain.com/{imageName}', 'detail': 'low'})], additional_kwargs={}), HumanMessage(content='Describe this image in brief', additional_kwargs={}, response_metadata={})])\n",
      "Prompt Value:  ChatPromptValue(messages=[HumanMessage(content=[{'type': 'image_url', 'image_url': {'url': 'http://yourdomain.com/my_pic.png', 'detail': 'low'}}], additional_kwargs={}, response_metadata={}), HumanMessage(content='Describe this image in brief', additional_kwargs={}, response_metadata={})])\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts.image import ImagePromptTemplate\n",
    "from langchain_core.prompts import HumanMessagePromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "# Lets create the Image Prompt Template\n",
    "imagePrompt = ImagePromptTemplate(input_variables=[\"imageName\"], template={\"url\":\"http://yourdomain.com/{imageName}\", \"detail\":\"low\"})\n",
    "\n",
    "# Just to check we are doing this right - lets invoke and see if the variables are replaced\n",
    "promptValue = imagePrompt.invoke(input={\"imageName\":\"good_pic.jpeg\"})\n",
    "print('Image Prompt',imagePrompt.__repr__())\n",
    "print('Prompt Value', promptValue.__repr__())\n",
    "print(\"ImageURL object\", promptValue.image_url)\n",
    "\n",
    "\n",
    "# Now lets wrap our Image Prompt Template within our Human Message Prompt Template\n",
    "humanMessage1 = HumanMessagePromptTemplate(prompt=[imagePrompt])\n",
    "print('Human Message1', humanMessage1.__repr__())\n",
    "\n",
    "# And since there is no replacement variable we create the message directly.\n",
    "humanMessage2 = HumanMessage(\"Describe this image in brief\")\n",
    "print('Human Message2', humanMessage2.__repr__())\n",
    "\n",
    "# Now we can use both these in our Chat Prompt Template\n",
    "chatPrompt = ChatPromptTemplate.from_messages([\n",
    "    humanMessage1, humanMessage2\n",
    "])\n",
    "print('Chat prompt: ', chatPrompt.__repr__())\n",
    "\n",
    "# And invoke will substitute the values and prepare the prompt to be sent to model\n",
    "promptValue = chatPrompt.invoke(input={\"imageName\":\"my_pic.png\"})\n",
    "print('Prompt Value: ',promptValue.__repr__())\n",
    "\n",
    "# model.invoke(promptValue) Try when you have an image.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompts with examples\n",
    "1. At times we want to provide examples with our prompts.\n",
    "2. Langchain has extensive support for examples.\n",
    "3. Check a useful diagram here: https://techblogs.cloudlex.com/langchain-iii-prompts-2df826c0ec3d#ea75#1dc7\n",
    "4. You can provide examples directly with your code or usually you want to pick examples from a store of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Please answer the following question based on the examples provided.\\n\\nInput: Do lions hunt in a pack? -> Output: Yes lions, unlike tigers hunt as a group\\n\\nInput: Who is the founder of Tesla Motors? -> Output: Elon Musk\\n\\nInput: What are the mountain ranges in west India? -> Output: They are western ghats\\n\\nInput: stoic -> Output: quiet\\n\\nInput: What is the capital of Japan -> Output: '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Based on the examples provided, the output would be:\\n\\nTōkyō', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 101, 'total_tokens': 117, 'completion_time': 0.013333333, 'prompt_time': 0.004531913, 'queue_time': 0.009829776, 'total_time': 0.017865246}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_179b0f92c9', 'finish_reason': 'stop', 'logprobs': None}, id='run-3e19a4f6-f0ec-46e1-9583-7d16a7102964-0', usage_metadata={'input_tokens': 101, 'output_tokens': 16, 'total_tokens': 117})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate, FewShotPromptTemplate\n",
    "examples = [\n",
    "    {\n",
    "        \"type\":\"animals\",\n",
    "        \"que\":\"Do lions hunt in a pack?\",\n",
    "        \"ans\":\"Yes lions, unlike tigers hunt as a group\"\n",
    "    },\n",
    "    {\n",
    "        \"type\":\"corporates\",\n",
    "        \"que\":\"Who is the founder of Tesla Motors?\",\n",
    "        \"ans\": \"Elon Musk\"\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"geography\",\n",
    "        \"que\":\"What are the mountain ranges in west India?\",\n",
    "        \"ans\": \"They are western ghats\"\n",
    "    },\n",
    "    {\n",
    "        \"type\":\"synonyms\",\n",
    "        \"que\":\"stoic\",\n",
    "        \"ans\":\"quiet\"\n",
    "    }\n",
    "]\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"que\":\"Do lions hunt in a pack?\",\n",
    "        \"ans\":\"Yes lions, unlike tigers hunt as a group\"\n",
    "    },\n",
    "    {\n",
    "        \"que\":\"Who is the founder of Tesla Motors?\",\n",
    "        \"ans\": \"Elon Musk\"\n",
    "    },\n",
    "    {\n",
    "        \"que\":\"What are the mountain ranges in west India?\",\n",
    "        \"ans\": \"They are western ghats\"\n",
    "    },\n",
    "    {\n",
    "        \"que\":\"stoic\",\n",
    "        \"ans\":\"quiet\"\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt = PromptTemplate.from_template(\"Input: {que} -> Output: {ans}\"),\n",
    "    prefix = \"Please answer the following question based on the examples provided.\",\n",
    "    suffix = \"Input: {user_question} -> Output: \",\n",
    "    input_variables=[\"user_question\"]\n",
    ")\n",
    "\n",
    "promptValue = prompt.format(user_question=\"What is the capital of Japan\")\n",
    "\n",
    "print(promptValue.__repr__())\n",
    "\n",
    "model.invoke(promptValue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user_question': 'What is the capital of Japan', 'typeOfQue': 'geography'}\n",
      "'Please answer the following question based on the examples provided.\\n\\nInput: What are the mountain ranges in west India? -> Output: They are western ghats\\n\\nInput: What is the capital of Japan -> Output: '\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, Dict, List\n",
    "from langchain_core.example_selectors import LengthBasedExampleSelector, BaseExampleSelector\n",
    "from langchain_core.prompts import PromptTemplate, FewShotPromptTemplate\n",
    "examples = [\n",
    "    {\n",
    "        \"type\":\"animals\",\n",
    "        \"que\":\"Do lions hunt in a pack?\",\n",
    "        \"ans\":\"Yes lions, unlike tigers hunt as a group\"\n",
    "    },\n",
    "    {\n",
    "        \"type\":\"corporates\",\n",
    "        \"que\":\"Who is the founder of Tesla Motors?\",\n",
    "        \"ans\": \"Elon Musk\"\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"geography\",\n",
    "        \"que\":\"What are the mountain ranges in west India?\",\n",
    "        \"ans\": \"They are western ghats\"\n",
    "    },\n",
    "    {\n",
    "        \"type\":\"synonyms\",\n",
    "        \"que\":\"stoic\",\n",
    "        \"ans\":\"quiet\"\n",
    "    }\n",
    "    \n",
    "]\n",
    "class MyExampleSelector(BaseExampleSelector):\n",
    "    def __init__(self, examples):\n",
    "        self.examples = examples\n",
    "    def add_example(self, example: Dict[str, str]) -> Any:\n",
    "        return super().add_example(example)\n",
    "\n",
    "    def select_examples(self, input_variables) -> List[dict]:\n",
    "        selected_examples = []\n",
    "        typeOfQue = input_variables[\"typeOfQue\"]\n",
    "        print(input_variables)\n",
    "        for example in self.examples:\n",
    "            if example[\"type\"] == typeOfQue:\n",
    "                selected_examples.append(example)\n",
    "\n",
    "        return selected_examples\n",
    "\n",
    "        \n",
    "# selector = LengthBasedExampleSelector(examples=examples, example_prompt= PromptTemplate.from_template(\"Input: {que} -> Output: {ans}\"), max_length=8)\n",
    "selector = MyExampleSelector(examples=examples)\n",
    "\n",
    "\n",
    "prompt = FewShotPromptTemplate(\n",
    "    example_selector=selector, # now instead of using examples directly we use selector\n",
    "    example_prompt = PromptTemplate.from_template(\"Input: {que} -> Output: {ans}\"),\n",
    "    prefix = \"Please answer the following question based on the examples provided.\",\n",
    "    suffix = \"Input: {user_question} -> Output: \",\n",
    "    input_variables=[\"user_question\", \"typeOfQue\"]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "promptValue = prompt.format(user_question=\"What is the capital of Japan\", typeOfQue=\"geography\")\n",
    "print(promptValue.__repr__())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'type': 'geography', 'que': 'What are the mountain ranges in west India?', 'ans': 'They are western ghats'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hhh'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"type\":\"animals\",\n",
    "        \"que\":\"Do lions hunt in a pack?\",\n",
    "        \"ans\":\"Yes lions, unlike tigers hunt as a group\"\n",
    "    },\n",
    "    {\n",
    "        \"type\":\"corporates\",\n",
    "        \"que\":\"Who is the founder of Tesla Motors?\",\n",
    "        \"ans\": \"Elon Musk\"\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"geography\",\n",
    "        \"que\":\"What are the mountain ranges in west India?\",\n",
    "        \"ans\": \"They are western ghats\"\n",
    "    },\n",
    "    {\n",
    "        \"type\":\"synonyms\",\n",
    "        \"que\":\"stoic\",\n",
    "        \"ans\":\"quiet\"\n",
    "    }\n",
    "    \n",
    "]\n",
    "l = []\n",
    "for example in examples:\n",
    "    if example[\"type\"] == \"geography\":\n",
    "        l.append(example)\n",
    "\n",
    "print(l)\n",
    "\n",
    "\"HHH\".lower()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
